{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e199c580",
   "metadata": {},
   "source": [
    "# Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b256797",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU ID:0,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%run /Desktop/Share/CUDA_DEVICE_setup.py -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d22264",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644a3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfdfb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 00:40:47.633653: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deepcalo as dpcal\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# ==============================================================================\n",
    "# Argument, for further available arguments please refer to the readme file\n",
    "# ==============================================================================\n",
    "\n",
    "exp_dir = 'exp_electrons/'\n",
    "data_path = '/Desktop/CodeFolder/crchen/MC_abseta_1.6_2.5_et_0.0_5000000.0_processes_energy.h5'\n",
    "rm_bad_reco = True\n",
    "zee_only = True\n",
    "\n",
    "gpu_ids = \"0\"\n",
    "apply_mask = rm_bad_reco or zee_only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cd7ca",
   "metadata": {},
   "source": [
    "# Import parameters config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c872a320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate range for learning rate schedule CLR set to (4.61880e-04, 1.38564e-03) by auto_lr.\n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "param_conf = import_module(exp_dir.split('/')[-2] + '.param_conf')\n",
    "params = param_conf.get_params()\n",
    "params = dpcal.utils.merge_dicts(dpcal.utils.get_default_params(), params, in_depth=True)\n",
    "if params['auto_lr']:\n",
    "    params = dpcal.utils.set_auto_lr(params)\n",
    "dirs = dpcal.utils.create_directories(exp_dir, params['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a322b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized config found:\n",
      "kernel_quantizer: \t quantized_bits(16,7,alpha=1)\n",
      "bias_quantizer: \t quantized_bits(16,7,alpha=1)\n",
      "activation_bits: \t quantized_bits(16,7,alpha=1)\n",
      "q_activation: \t\t quantized_relu(bits=16, integer=8)\n",
      "final_q_activation: \t quantized_relu(bits=32, integer=16)\n",
      "max_quantized_value: \t 128.0\n",
      "min_quantized_value: \t -128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 00:40:49.814813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-12-02 00:40:49.815701: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-02 00:40:49.816504: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "if 'quantizer_config' in params:\n",
    "    print(\"quantized config found:\")\n",
    "    print(\"kernel_quantizer: \\t\", params['quantizer_config']['kernel_quantizer'])\n",
    "    print(\"bias_quantizer: \\t\", params['quantizer_config']['bias_quantizer'])\n",
    "    print(\"activation_bits: \\t\", params['quantizer_config']['activation_bits'])\n",
    "    print(\"q_activation: \\t\\t\", params['quantizer_config']['q_activation'])\n",
    "    print(\"final_q_activation: \\t\", params['quantizer_config']['final_q_activation'])\n",
    "    from qkeras import QActivation\n",
    "    max_quantized_value = QActivation(params['quantizer_config']['activation_bits']).quantizer.max()\n",
    "    min_quantized_value = QActivation(params['quantizer_config']['activation_bits']).quantizer.min()\n",
    "    print(\"max_quantized_value: \\t\", max_quantized_value)\n",
    "    print(\"min_quantized_value: \\t\", min_quantized_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66556e2",
   "metadata": {},
   "source": [
    "# Standardize scalars and mask tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011fc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "_n_train = 8e5\n",
    "_n_val = -1\n",
    "_n_test = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2376e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 800000.0, 'val': None, 'test': None}\n",
      "Loading data.\n",
      "Loading only the 800000 first data points of the train set.\n",
      "Since using hls4ml later, Targets will be scaled down by 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_points = {'train':None, 'val':None, 'test':None}\n",
    "for set_name, n in zip(n_points, [_n_train, _n_val, _n_test]):\n",
    "    if not n < 0:\n",
    "        n_points[set_name] = n\n",
    "print(n_points)\n",
    "if params['data_generator']['use']:\n",
    "    params['data_generator']['n_points'] = n_points\n",
    "    n_points = {set_name:1 for set_name in n_points}\n",
    "    if data_path is not None:\n",
    "        params['data_generator']['path'] = data_path\n",
    "    data = dpcal.utils.load_atlas_data(n_points=n_points,\n",
    "                                       **params['data_generator']['load_kwargs'],\n",
    "                                       verbose=False)\n",
    "else:\n",
    "    # Import data parameters (what should or should not be loaded) as a module\n",
    "    data_conf = import_module(exp_dir.split('/')[-2] + '.data_conf')\n",
    "    data_params = data_conf.get_params()\n",
    "\n",
    "    # Load the data\n",
    "    data = dpcal.utils.load_atlas_data(path=data_path, n_points=n_points, use_hls4ml = params['use_hls4ml'], **data_params)\n",
    "scalars_to_quantile = ['p_pt_track', 'p_R12', 'p_deltaPhiRescaled2', 'p_deltaEta2',\n",
    "                        'p_f0Cluster', 'p_eAccCluster', 'p_photonConversionRadius',]\n",
    "\n",
    "# Standardize scalars and tracks\n",
    "for dataset_name in ['scalars']:\n",
    "\n",
    "    if dataset_name=='scalars':\n",
    "        data_param_name = 'scalar_names'\n",
    "    elif dataset_name=='tracks':\n",
    "        data_param_name = 'track_names'\n",
    "    if data_params[data_param_name] is None:\n",
    "        continue\n",
    "\n",
    "    # Make directory for saving scalers\n",
    "    scaler_dir = dirs['log'] + 'scalers/'\n",
    "    if not os.path.exists(scaler_dir):\n",
    "        os.makedirs(scaler_dir)\n",
    "\n",
    "    for name in data_params[data_param_name]:\n",
    "        # Get index of variable\n",
    "        var_ind = data_params[data_param_name].index(name)\n",
    "\n",
    "        # Get which scaler to use\n",
    "        scaler_name = 'Quantile' if name in scalars_to_quantile else 'Robust'\n",
    "\n",
    "        # Standardize in-place\n",
    "        dpcal.utils.standardize(data,\n",
    "                                dataset_name,\n",
    "                                variable_index=var_ind,\n",
    "                                scaler_name=scaler_name,\n",
    "                                save_path=os.path.join(scaler_dir, f'scaler_{scaler_name}_{name}.jbl'))\n",
    "\n",
    "        if True:\n",
    "            print(f'Standardizing {name} with {scaler_name}Scaler')\n",
    "            for set_name in data:\n",
    "                print(f'Min and max of {name} in {set_name} set after standardization: '\n",
    "                      f'{data[set_name][dataset_name][:,var_ind].min(), data[set_name][dataset_name][:,var_ind].max()}')\n",
    "                \n",
    "if not params['data_generator']['use']:\n",
    "\n",
    "    if apply_mask:\n",
    "        # Make masks\n",
    "        mask = dpcal.utils.make_mask(data_path, n_points, rm_bad_reco=rm_bad_reco,\n",
    "                                     zee_only=zee_only, lh_cut_name=None,\n",
    "                                     rm_conv=None, rm_unconv=None)\n",
    "\n",
    "        # Apply mask\n",
    "        data = dpcal.utils.apply_mask(data, mask, skip_name=None)\n",
    "        \n",
    "        # set apply_mask to False in parameters since it is aleardy masked\n",
    "        params['track_net']['apply_mask'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6fe96",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67621c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images:\n",
    "\n",
    "for set_name in n_points:\n",
    "    imgs = data[set_name]['images']['em_endcap']\n",
    "    imgs_thr = np.percentile(np.max(imgs, axis=(1,2,3)) ,95)\n",
    "    imgs = np.clip(imgs, None, imgs_thr)\n",
    "    data[set_name]['images']['em_endcap'] = MinMaxScaler().fit_transform(imgs.reshape(imgs.shape[0], -1)).reshape(imgs.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ca09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalars:\n",
    "\n",
    "for set_name in n_points:\n",
    "    data_pre = data[set_name]['scalars']\n",
    "    data_pre_thr = np.percentile(np.max(data_pre, axis=1) ,95)\n",
    "    data_pre = np.clip(data_pre, None, data_pre_thr)\n",
    "    data[set_name]['scalars'] = MinMaxScaler().fit_transform(data_pre.reshape(data_pre.shape[0], -1)).reshape(data_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc666747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks:\n",
    "\n",
    "for set_name in n_points:\n",
    "    data_pre = data[set_name]['tracks']\n",
    "    data_pre_thr = np.percentile(np.max(data_pre, axis=(1,2)) ,95)\n",
    "    data_pre = np.clip(data_pre, None, data_pre_thr)\n",
    "    data[set_name]['tracks'] = MinMaxScaler().fit_transform(data_pre.reshape(data_pre.shape[0], -1)).reshape(data_pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb6341",
   "metadata": {},
   "source": [
    "# Plot max value in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_imgs = data[\"train\"]['images']['em_endcap']\n",
    "tr_scalars = data[\"train\"]['scalars']\n",
    "tr_tracks = data[\"train\"]['tracks']\n",
    "tr_targets = data[\"train\"]['targets']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622dee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Min and max of images: \", np.min(tr_imgs), np.max(tr_imgs))\n",
    "print(\"Min and max of scalars: \", np.min(tr_scalars), np.max(tr_scalars))\n",
    "print(\"Min and max of tracks: \", np.min(tr_tracks), np.max(tr_tracks))\n",
    "print(\"Min and max of targets: \", np.min(tr_targets), np.max(tr_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_tracks_bool = np.max(tr_tracks, axis=(1,2)) > np.percentile(np.max(tr_tracks, axis=(1,2)) ,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_tracks = np.delete(tr_tracks, tr_tracks_bool,axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09997334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#q95,q5 = np.percentile(tr_imgs,[95,5])\n",
    "#intr_qr = q95-q5\n",
    "#print(intr_qr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 30\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.plot(np.max(tr_imgs, axis=(1,2,3)) )\n",
    "plt.title('max value in images/batch', fontsize = fontsize)\n",
    "plt.ylabel('value', fontsize = fontsize)\n",
    "plt.xlabel('batch', fontsize = fontsize)\n",
    "plt.savefig(\"tr_images.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c45222",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 30\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.plot(np.max(tr_tracks, axis=(1,2)) )\n",
    "plt.title('max value in tracks/batch', fontsize = fontsize)\n",
    "plt.ylabel('value', fontsize = fontsize)\n",
    "plt.xlabel('batch', fontsize = fontsize)\n",
    "plt.savefig(\"tr_tracks.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 30\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.plot(np.max(tr_scalars, axis=1) )\n",
    "plt.title('max value in scalars/batch', fontsize = fontsize)\n",
    "plt.ylabel('value', fontsize = fontsize)\n",
    "plt.xlabel('batch', fontsize = fontsize)\n",
    "plt.savefig(\"tr_scalars.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 30\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.plot(tr_targets)\n",
    "plt.title('max value in targets/batch', fontsize = fontsize)\n",
    "plt.ylabel('value', fontsize = fontsize)\n",
    "plt.xlabel('batch', fontsize = fontsize)\n",
    "plt.savefig(\"tr_targets.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = dpcal.ModelContainer(data=data,\n",
    "                          params=params,\n",
    "                          dirs=dirs,\n",
    "                          save_figs=False,\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170790df",
   "metadata": {},
   "source": [
    "# Qmodel_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228204b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print_qmodel_summary(mc.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1645ae",
   "metadata": {},
   "source": [
    "# Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "dot_img_file = 'dcalo_full_qkeras_qdensebatchnorm.png'\n",
    "plot_model(mc.model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ac6f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = mc.train()\n",
    "\n",
    "# Evaluate (predicting and evaluating on test or validation set)\n",
    "if not hasattr(mc,'evaluation_scores'):\n",
    "    mc.evaluate()\n",
    "\n",
    "# Print results\n",
    "print('Evaluation scores:')\n",
    "print(mc.evaluation_scores)\n",
    "\n",
    "\n",
    "\n",
    "qmodel = mc.model\n",
    "print('final model saved')\n",
    "qmodel.save('./model_weight/keras_qmodel_endcap_{}.h5'.format(params['quantizer_config']['activation_bits']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input images\n",
    "x_test_em_barrel = data[set_name]['images']['em_barrel']\n",
    "#np.save('x_test_em_barrel.npy', x_test_em_barrel)\n",
    "print('x_test_em_barrel shape: ', x_test_em_barrel.shape)\n",
    "#print('x_test_em_barrel saved')\n",
    "\n",
    "#input scalar\n",
    "x_test_scalars = data[set_name]['scalars']\n",
    "#np.save('x_test_scalars.npy', x_test_scalars)\n",
    "print('x_test_scalars shape: ', x_test_scalars.shape)\n",
    "#print('x_test_scalars saved')\n",
    "\n",
    "#input track\n",
    "x_test_tracks = data[set_name]['tracks']\n",
    "#np.save('x_test_tracks.npy', x_test_tracks)\n",
    "print('x_test_tracksl shape: ', x_test_tracks.shape)\n",
    "#print('x_test_tracks saved')\n",
    "\n",
    "#target\n",
    "y_test_targets = data[set_name]['targets']\n",
    "#np.save('y_test_targets.npy', y_test_targets)\n",
    "print('y_test_targets shape: ', y_test_targets.shape)\n",
    "#print('y_test_targets saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [x_test_em_barrel, x_test_scalars, x_test_tracks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e59ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.model.evaluate(x_test, y_test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e604403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim(0, 80)\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('model_loss_300ep_keras_qmodel_endcap_17_9.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeca57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantized_model_debug(mc.model, x_test, plot=True, plt_instance=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511af9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
